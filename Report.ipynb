{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Visual Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "1. Julia Lorenz, 156066\n",
    "2. Marcel Rojewski, 156059\n",
    "\n",
    "Github repo: https://github.com/marcelrojo/SearchEngine-CV.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "The dataset used is a subset of the **Caltech 256 Image Dataset**, originally sourced from [Kaggle](https://www.kaggle.com/datasets/jessicali9530/caltech256/data). While the full dataset includes 257 object categories and a total of 30,607 images, for this project, we have selected 100 categories from the Caltech 256 dataset and added 6 custom object categories. Each category contains approximately 100 images. And in total we have 11,520 images.\n",
    "\n",
    "The dataset features a broad range of categories, including everyday objects and more unusual items, such as backpacks, bulldozers, rubber ducks, fried eggs, and diamond rings. These objects vary in size and appearance, providing a rich and diverse collection for training and evaluating models.\n",
    "\n",
    "Below are some examples of the object categories included in the dataset:\n",
    "\n",
    "- **Fireworks**\n",
    "\n",
    "  <img src=\"images/fireworks.png\" alt=\"Three examples from fireworks\" width=\"800\">\n",
    "  \n",
    "- **Baseball Bats**\n",
    "\n",
    "  <img src=\"images/baseball_bats.png\" alt=\"Three examples from baseball-bats\" width=\"800\">\n",
    "  \n",
    "- **Kites**\n",
    "\n",
    "  <img src=\"images/kites.png\" alt=\"Three examples from kites\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem\n",
    "\n",
    "The task is to build a visual search engine that, given an image query, retrieves the top `k` most similar images from a dataset. This involves training a model to extract meaningful features (embeddings) from images, which can then be used to compare and rank other images in the dataset based on their similarity to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.1 Architectures - Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Neural Network\n",
    "A **Siamese Neural Network** is a special type of neural network architecture designed to compare pairs of images. It learns to identify the similarities or differences between the inputs by training on labeled pairs of images. \n",
    "\n",
    "Our idea was to create a Siamese network that learns to classify pairs of images as belonging to the same class (label 1.0) or from different classes (label 0.0). This process involved the following:\n",
    "\n",
    "<img src=\"images/siamese_outputs/siamese_network_diagram.png\" alt=\"Siamese network diagram\" width=\"400\">\n",
    "\n",
    "1. **Siamese Model Architecture** consisted of two identical base networks that share weights. The two input images were processed independently through these base networks, and the output embeddings were compared using an L2 distance metric. The model predicted the distance between the embeddings, which is then used to determine whether the images are similar or not.\n",
    "2. **Base Network for Feature Extraction** was designed to extract relevant features from images and generate a 128-dimensional feature vector for each image. After training the Siamese network, we extracted the base model and used its output to create embeddings for new images.\n",
    "3. **Contrastive Loss Function** was implemented to train the Siamese network, that aimed to encourage the network to output samller distances for similar pairs.\n",
    "4. **Custom Accuracy Metric**, since the network outputs distance values, we introduced a custom accuracy function. This function assumes a threshold and converts the output distances into binary values (0 or 1) based on whether they are smaller or larger than the threshold, respectively. This allows the model's predictions to be evaluated in terms of accuracy.\n",
    "5. **Comparison and Similarity**. After training the model, we extrected the base model to obtain embeddings for the database images and predict embeddings for the query image. After obtaining the embeddings, the cosine distance between the two feature vectors was computed to measure their similarity. The top k most similar images were selected based on this distance.\n",
    "\n",
    "\n",
    "\n",
    "Although the model showed promising results, the model's performance was unstable and did not meet our expectations. Possibly due to inadequate data and model overfitting.\n",
    "\n",
    "Below are presented some examples produced by this approach:\n",
    "\n",
    "<img src=\"images/siamese_outputs/output1.png\" alt=\"Siamese network example output\" width=\"800\">\n",
    "\n",
    "<img src=\"images/siamese_outputs/output2.png\" alt=\"Siamese network example output\" width=\"800\">\n",
    "\n",
    "<img src=\"images/siamese_outputs/output3.png\" alt=\"Siamese network example output\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Analysis**\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,064,128</span> (129.94 MB)\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,064,128</span> (129.94 MB)\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model that will be used to extract features from the images, creates a feature vector of size 128\n",
    "def build_base_network(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (7, 7), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (5, 5), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(128, activation=\"relu\")(x) # 128-dimensional feature vector\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# full siamese network that takes two images as input and outputs the L2 distance between the feature vectors of the two images\n",
    "def build_siamese_network(input_shape):\n",
    "    base_network = build_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    \n",
    "    encoded_a = base_network(input_a)\n",
    "    encoded_b = base_network(input_b)\n",
    "    \n",
    "    # compute the L2 distance between the encoded vectors\n",
    "    l2_distance = Lambda(lambda tensors: K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=-1)))([encoded_a, encoded_b])\n",
    "    \n",
    "    return Model(inputs=[input_a, input_b], outputs=l2_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Architectures - Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Convolutional Neural Network \n",
    "\n",
    "We developed a **convolutional neural network (CNN)** designed specifically to classify images into 106 distinct categories.\n",
    "\n",
    "<img src=\"images/classification_outputs/classification_network_diagram.png\" alt=\"CNN diagram\" width=\"300\">\n",
    "\n",
    "This approach consisted of steps:\n",
    "1. **Train and Validation Split** The dataset was divided into training and validation sets to facilitate the training process. Since the test data will be provided separately for querying the application, we excluded a test split during this stage. For each class (corresponding to a folder), 90% of the images were used for training, and 10% were allocated for validation. At this point each image was assigned a label derived from the folder name (e.g., `001`, `002`, etc.).\n",
    "2. **Data Generator**, a custom data generator was implemented to preprocess images during training. This generator handled the following tasks:\n",
    "   - **Loading and Normalization**: Images were read and normalized to have pixel values in the desired range.\n",
    "   - **Resizing**: Images were resized by cropping instead of stretching to preserve the original aspect ratio.  \n",
    "   - **Augmentation**: Gaussian blur was applied to enhance robustness to noise.  \n",
    "   - **Batch Generation**: The generator returned batches of preprocessed images and their corresponding one-hot encoded labels. Shuffling was applied at the end of each epoch to ensure randomness and reduce overfitting. \n",
    "3. **Classification CNN Model**, the CNN model was designed to extract features and classify images into their respective categories. The architecture consisted of convolutional and pooling layers, followed by fully connected layers for classification. The final output layer used a **softmax activation** to produce a probability distribution over the 106 categories.  \n",
    "4. **Embedding Model Extraction**, to facilitate similarity-based querying, we extracted the second-to-last layer of the trained classification model. This layer produced a **512-dimensional feature vector** (embedding) for each image.\n",
    "5. **Similarity Computation**, once embeddings were generated, we used them for similarity-based image retrieval.  \n",
    "   - For a given query image, its embedding was predicted using the embedding model.  \n",
    "   - **Cosine similarity** was computed between the query embedding and all database embeddings.  \n",
    "   - The top-`k` most similar images were identified and returned based on the similarity scores.\n",
    "\n",
    "This approach achieved satisfactory performance, with the CNN producing accurate classifications in most cases. When used for similarity-based image retrieval, the results were generally very reliable. The retrieved images often belonged to the same category as the query, and even in cases where they did not, the retrieved images were visually similar. \n",
    "\n",
    "Here are a few example outputs demonstrating the results:\n",
    "\n",
    "<img src=\"images/classification_outputs/output2.png\" alt=\"CNN output\" width=\"800\">\n",
    "\n",
    "<img src=\"images/classification_outputs/output6.png\" alt=\"CNN output\" width=\"800\">\n",
    "\n",
    "<img src=\"images/classification_outputs/output8.png\" alt=\"CNN output\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classification_network(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)  \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)  \n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy',\n",
    "                                                                              Precision(name='precision'),\n",
    "                                                                              Recall(name='recall')])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Analysis**\n",
    "\n",
    "- **Memory Size**: The model occupies approximately 7.16 MB of memory.  \n",
    "- **Total Parameters**: The model has a total of 1,878,234 parameters.  \n",
    "- **Trainable Parameters**: Out of the total, 1,877,530 parameters are trainable.  \n",
    "- **Non-Trainable Parameters**: There are 704 non-trainable parameters, which are responsible for fixed operations such as batch normalization statistics.\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,878,234</span> (7.16 MB)\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,877,530</span> (7.16 MB)\n",
    "</pre>\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Analysis**\n",
    "\n",
    "**Epoch 12** - The learning rate was adjusted by the scheduler.\n",
    "\n",
    "A callback function was used to monitor the validation loss and restore the model's best weights.\n",
    "\n",
    "| ![CNN loss](images/classification_outputs/loss.png) | ![CNN accuracy](images/classification_outputs/accuracy.png) |\n",
    "|-----------------------------------------------------|------------------------------------------------------------|\n",
    "| ![CNN precision](images/classification_outputs/precision.png) | ![CNN recall](images/classification_outputs/recall.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**  \n",
    "\n",
    "- **Categorical Crossentropy** - used as the loss function for multi-class classification.  \n",
    "- **Accuracy** - measures the percentage of correct predictions.  \n",
    "- **Precision** - evaluates the proportion of true positive predictions among all positive predictions.  \n",
    "- **Recall** - assesses the proportion of true positives identified from all actual positives.  \n",
    "\n",
    "\n",
    "**Hyperperameters**\n",
    "\n",
    "- **Optimizer Hyperparameters** \n",
    "    - **Learning Rate** - statred with the default value of 0.001, and a shceduler was introduced, to reduce the learning rate by factor of 0.2 on plateu.\n",
    "- **Training Hyperparameters** \n",
    "    - **Batch Size** - a batch size of 32 was used, as it is commonly used, due to the fact that larger batches may increase the memory usage, and size 32 shown to be sufficient, smaller batches may introduce more noise.\n",
    "    - **Number of Epochs** - the number of epochs was set to 50, but due to early stopping monitoring the validation loss, the training process halted eariler.\n",
    "- **Callbacks**\n",
    "    - **EarlyStopping** - stops the training and restores best weights.\n",
    "    - **ReduceLROnPlateau** - reduces the learning rate.\n",
    "     \n",
    "\n",
    "**Inference Time** - 2.384185791015625e-07 s (predict embedding for a query)\n",
    "\n",
    "**Training Time** - 75s * 18 epochs = approx 22.5 minutes\n",
    "\n",
    "**Runtime Environment** The model was trained in a Kaggle runtime environment utilizing dual NVIDIA Tesla T4 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Items\n",
    "\n",
    "| Item | Points |\n",
    "|----------|----------|\n",
    "| Problem (Search Engine) | 2 | \n",
    "| Model Own Architecture (Classification CNN) | 2 | \n",
    "| Model Own Architecture (Siamese NN) | 2 | \n",
    "| Model Non-Trivial Solution (Contrastive Learning - Siamese NN) | 1 | \n",
    "| Dataset (10,000+ photos) | 1 | \n",
    "| Dataset (Own part of dataset 500>) | 1 | \n",
    "| Architecture Tuning (3 architectures) | 1 | \n",
    "| Tools (GUI) | 1 | \n",
    "| | | \n",
    "| | | \n",
    "| | | \n",
    "| | | \n",
    "| | | \n",
    "| | | \n",
    "| | | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Dataset: \n",
    "\n",
    "    https://www.kaggle.com/datasets/jessicali9530/caltech256/data\n",
    "\n",
    "    Griffin, G., Holub, A., & Perona, P. (2022). Caltech 256 (1.0) [Data set]. CaltechDATA. https://doi.org/10.22002/D1.20087\n",
    "\n",
    "- Siamese Network Guide\n",
    "\n",
    "    https://builtin.com/machine-learning/siamese-network\n",
    "    \n",
    "    https://medium.com/@rinkinag24/a-comprehensive-guide-to-siamese-neural-networks-3358658c0513"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
